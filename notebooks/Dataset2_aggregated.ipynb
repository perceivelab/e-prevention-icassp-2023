{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track 2 (aggregated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s_calcagno/miniconda3/envs/pytorch/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import pyhrv\n",
    "import scipy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from monai.config import KeysCollection\n",
    "from monai.transforms import MapTransform\n",
    "from monai.transforms import Compose, ToTensorD\n",
    "from monai.data import CacheDataset, DataLoader, DistributedSampler\n",
    "\n",
    "valid_range = {\n",
    "    \"acc_X\" : (-19.6, 19.6),\n",
    "    \"acc_Y\" : (-19.6, 19.6),\n",
    "    \"acc_Z\" : (-19.6, 19.6),\n",
    "    \"gyr_X\" : (-573, 573),\n",
    "    \"gyr_Y\" : (-573, 573),\n",
    "    \"gyr_Z\" : (-573, 573),\n",
    "    \"heartRate\" : (0, 255),\n",
    "    \"rRInterval\" : (0, 2000),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_mask(slice, keys):\n",
    "    valid_mask = pd.Series([True]*len(slice))\n",
    "    valid_mask.index = slice.index\n",
    "    for k in keys:\n",
    "        a, b = valid_range[k]\n",
    "        valid_mask = valid_mask & (slice[k]>a) & (slice[k]<=b)\n",
    "    return valid_mask\n",
    "\n",
    "def mean_acc_norm(slice):\n",
    "    # Compute the mean norm of accelerometer\n",
    "    keys = ['acc_X', 'acc_Y', 'acc_Z']\n",
    "    valid_mask = get_valid_mask(slice, keys)\n",
    "    if not valid_mask.any():\n",
    "        return np.nan\n",
    "    acc_norm = slice[valid_mask][keys].apply(lambda x: np.linalg.norm(x), axis=1).to_numpy()\n",
    "    return acc_norm.mean()\n",
    "\n",
    "def mean_gyr_norm(slice):    \n",
    "    # Compute the mean norm of gyroscope\n",
    "    keys = ['gyr_X', 'gyr_Y', 'gyr_Z']\n",
    "    valid_mask = get_valid_mask(slice, keys)\n",
    "    if not valid_mask.any():\n",
    "        return np.nan\n",
    "    gyr_norm = slice[valid_mask][keys].apply(lambda x: np.linalg.norm(x), axis=1).to_numpy()\n",
    "    return gyr_norm.mean()\n",
    "\n",
    "def mean_hrv(slice):\n",
    "    # Compute the mean HRV\n",
    "    key = 'heartRate'\n",
    "    valid_mask = get_valid_mask(slice, [key])\n",
    "    if not valid_mask.any():\n",
    "        return np.nan\n",
    "    return slice[valid_mask][key].mean()\n",
    "\n",
    "def mean_rr(slice):\n",
    "    # Compute the mean RR interval\n",
    "    key = 'rRInterval'\n",
    "    valid_mask = get_valid_mask(slice, [key])\n",
    "    if not valid_mask.any():\n",
    "        return np.nan\n",
    "    return slice[valid_mask][key].mean()\n",
    "\n",
    "def poincare_major_axis(slice):\n",
    "    # Compute the Major Axis of PoincarÃ¨ Plot\n",
    "    keys = ['rRInterval']\n",
    "    valid_mask = get_valid_mask(slice, keys)\n",
    "    if not valid_mask.any():\n",
    "        return np.nan\n",
    "    plt.ioff()\n",
    "    return pyhrv.nonlinear.poincare(slice[valid_mask][keys].to_numpy(), show=False)['sd2']\n",
    "\n",
    "def lombscargle_power(slice):\n",
    "    # Compute the Low  and High Frequency Power of Lomb-Scargle Periodogram\n",
    "    key = 'rRInterval'\n",
    "    valid_mask = get_valid_mask(slice, [key])\n",
    "    if not valid_mask.any():\n",
    "        return np.nan, np.nan\n",
    "    nni = slice[valid_mask][key]\n",
    "    # low frequencies\n",
    "    l = 0.04 * np.pi /2\n",
    "    h = 0.15 * np.pi /2\n",
    "    freqs = np.linspace(l, h, 1000)\n",
    "    lf_lsp = scipy.signal.lombscargle(nni.to_numpy(), nni.index.to_numpy(), freqs, normalize=True)\n",
    "    # high frequencies\n",
    "    l = 0.15 * np.pi /2\n",
    "    h = 0.4 * np.pi /2\n",
    "    freqs = np.linspace(l, h, 1000)\n",
    "    hf_lsp = scipy.signal.lombscargle(nni.to_numpy(), nni.index.to_numpy(), freqs, normalize=True)\n",
    "    return np.trapz(lf_lsp, freqs), np.trapz(hf_lsp, freqs)\n",
    "\n",
    "def time_encoding(slice):\n",
    "    # Compute the sin and cos of timestamp (we have 12*24=288 5-minutes per day)\n",
    "    mean_timestamp = slice['timecol'].astype('datetime64').mean()\n",
    "    h = mean_timestamp.hour\n",
    "    m = mean_timestamp.minute\n",
    "    time_value = h*60 + m\n",
    "    sin_t = np.sin(time_value*(2.*np.pi/(60*24)))\n",
    "    cos_t = np.cos(time_value*(2.*np.pi/(60*24)))\n",
    "    return sin_t, cos_t\n",
    "\n",
    "def validity_percentage(slice):\n",
    "    # Compute the % of valid data\n",
    "    validity = []\n",
    "    for k, (a, b) in valid_range.items():\n",
    "        valid_mask = (slice[k] > a) & (slice[k] <= b)\n",
    "        try:\n",
    "            num_valid = valid_mask.value_counts()[True]\n",
    "        except KeyError:\n",
    "            num_valid = 0\n",
    "        percent_valid = num_valid/len(valid_mask)\n",
    "        validity.append(percent_valid)\n",
    "        return np.array(validity).mean()\n",
    "\n",
    "def extract_features(slice, return_numpy=False):\n",
    "    if return_numpy:\n",
    "        return np.array([\n",
    "            mean_acc_norm(slice),\n",
    "            mean_gyr_norm(slice),\n",
    "            mean_hrv(slice),\n",
    "            mean_rr(slice),\n",
    "            poincare_major_axis(slice),\n",
    "            *lombscargle_power(slice),\n",
    "            *time_encoding(slice),\n",
    "            validity_percentage(slice),\n",
    "        ])\n",
    "    l, h = lombscargle_power(slice)\n",
    "    sin, cos = time_encoding(slice)\n",
    "    return {\n",
    "            'mean_acc_norm' : mean_acc_norm(slice),\n",
    "            'mean_gyr_norm' : mean_gyr_norm(slice),\n",
    "            'mean_hrv' : mean_hrv(slice),\n",
    "            'mean_rr' : mean_rr(slice),\n",
    "            'poincare_major_axis' : poincare_major_axis(slice),\n",
    "            'lombscargle_power_low' : l,\n",
    "            'lombscargle_power_high' : h,\n",
    "            'sin_time_encoding' : sin,\n",
    "            'cos_time_encoding' : cos,\n",
    "            'validity_percentage' : validity_percentage(slice),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"../../datasets/SPGC_challenge_track_2_release/\")\n",
    "\n",
    "def get_paths(root_dir, split):\n",
    "    paths = []\n",
    "    if split != 'test':\n",
    "        base_dir = Path('training_data')\n",
    "        for user in os.listdir(root_dir/base_dir):\n",
    "            user_dir = base_dir/Path(user)/Path(split)\n",
    "            for status in os.listdir(root_dir/user_dir):\n",
    "                status_dir = user_dir/Path(status)\n",
    "                for sample in os.listdir(root_dir/status_dir):\n",
    "                    paths.append(status_dir/Path(sample))\n",
    "    else:\n",
    "        base_dir = Path('test_data')\n",
    "        for user in os.listdir(root_dir/base_dir):\n",
    "            user_dir = base_dir/Path(user)/Path(split)\n",
    "            for sample in os.listdir(root_dir/user_dir):\n",
    "                paths.append(user_dir/Path(sample))\n",
    "    return paths\n",
    "\n",
    "def parse_path(path):\n",
    "    path = str(path).split(\"/\")\n",
    "    user = int(path[1].split(\"_\")[1])\n",
    "    split = path[2]\n",
    "    if len(path)==5:\n",
    "        status = 1 if path[3]=='relapse' else 0\n",
    "        id = int(path[4])\n",
    "    else:\n",
    "        status = -1\n",
    "        id = int(path[3])\n",
    "    return user, split, status, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd598567e544ffa93076d2f26a615d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d5257f391e499cafe8797c0645d489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f1fe96945343a89a6dba16afee3512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/544 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paths = {split: get_paths(root_dir, split) for split in ['train', 'val', 'test']}\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for sample in tqdm(paths[split]):\n",
    "        if len(os.listdir(root_dir/sample)) < 3:\n",
    "            data = pd.read_csv(root_dir/sample/\"data.csv\")\n",
    "            # interate over data\n",
    "            slices = []\n",
    "            for start in range(0, len(data), 12*5):\n",
    "                # Extract a 5-minutes slices\n",
    "                slice = data.loc[start:start+59]\n",
    "                slices.append(extract_features(slice))\n",
    "            pd.DataFrame().from_records(slices).to_csv(root_dir/sample/\"aggregated_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: user_02, Status: non-relapse, Samples:25\n",
      "User: user_02, Status: relapse, Samples:13\n",
      "User: user_08, Status: non-relapse, Samples:13\n",
      "User: user_08, Status: relapse, Samples:3\n",
      "User: user_00, Status: non-relapse, Samples:31\n",
      "User: user_00, Status: relapse, Samples:9\n",
      "User: user_01, Status: non-relapse, Samples:22\n",
      "User: user_01, Status: relapse, Samples:57\n",
      "User: user_09, Status: non-relapse, Samples:21\n",
      "User: user_09, Status: relapse, Samples:73\n",
      "User: user_05, Status: non-relapse, Samples:27\n",
      "User: user_05, Status: relapse, Samples:22\n",
      "User: user_07, Status: non-relapse, Samples:29\n",
      "User: user_07, Status: relapse, Samples:93\n",
      "User: user_06, Status: non-relapse, Samples:26\n",
      "User: user_06, Status: relapse, Samples:4\n",
      "User: user_04, Status: non-relapse, Samples:22\n",
      "User: user_04, Status: relapse, Samples:3\n",
      "User: user_03, Status: non-relapse, Samples:21\n",
      "User: user_03, Status: relapse, Samples:17\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path('training_data')\n",
    "split = 'val'\n",
    "for user in os.listdir(root_dir/base_dir):\n",
    "    user_dir = base_dir/Path(user)/Path(split)\n",
    "    for status in os.listdir(root_dir/user_dir):\n",
    "        print(f\"User: {user}, Status: {status}, Samples:{len(os.listdir(root_dir/user_dir/Path(status)))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"../../datasets/SPGC_challenge_track_2_release/\")\n",
    "splits = ['train', 'val', 'test']\n",
    "paths = {split: get_paths(root_dir, split) for split in splits}\n",
    "\n",
    "def validate(window):\n",
    "    invalid_filter = window.isna().any(axis=1)\n",
    "    return 1- (len(window[invalid_filter])/len(window)) \n",
    "\n",
    "def get_observations(root_dir, path, w_size_h=4, w_stride_h=1, val_percentage=0.25):\n",
    "    \n",
    "    data = pd.read_csv(root_dir/path/\"aggregated_data.csv\")\n",
    "    user, split, status, id = parse_path(path)\n",
    "    w_size = int(w_size_h*12)\n",
    "    w_stride = int(w_stride_h*12)\n",
    "    obs = []\n",
    "    path = Path(path/\"aggregated_data.csv\")\n",
    "    # Treat short sequences\n",
    "    if len(data) < w_size:\n",
    "        if split == 'train':\n",
    "            return obs\n",
    "        # Consider short windows in validation and test\n",
    "        else:\n",
    "            validity = validate(data)\n",
    "            return [{\n",
    "                'data_file' : path,\n",
    "                'user_id' : user,\n",
    "                'sample_id' : id,\n",
    "                'label' : status,\n",
    "                'valid' : validity >= val_percentage,\n",
    "                'start_data_row' : 0,\n",
    "                'end_data_row' : len(data) \n",
    "            }]\n",
    "    \n",
    "    # Slide windows\n",
    "    for start in range(0, len(data)-w_size, w_stride):\n",
    "        stop = start + w_size # excluded\n",
    "        window = data.loc[start:stop-1] # upperbound is included\n",
    "        # check validity\n",
    "        validity = validate(window)\n",
    "        obs.append({\n",
    "            'data_file' : path,\n",
    "            'user_id' : user,\n",
    "            'sample_id' : id,\n",
    "            'label' : status,\n",
    "            'valid' : validity >= val_percentage,\n",
    "            'start_data_row' : start,\n",
    "            'end_data_row' :stop\n",
    "        })\n",
    "\n",
    "    return obs\n",
    "\n",
    "def create_dataset_list(root_dir, paths,  w_size_h=4, w_stride_h=1, val_percentage=0.25):\n",
    "    dataset_list = []\n",
    "    for sample in paths:\n",
    "        # open file\n",
    "        dataset_list.extend(get_observations(root_dir, sample, w_size_h=w_size_h, w_stride_h=w_stride_h, val_percentage=val_percentage))\n",
    "    return dataset_list\n",
    "\n",
    "def _create_offsets(x):\n",
    "    if len(x[x.valid]) == 0:\n",
    "        return list(zip(x.start_data_row, x.end_data_row))\n",
    "    return list(zip(x[x.valid].start_data_row, x[x.valid].end_data_row))\n",
    "\n",
    "def save_dataset(root_dir, output_dir, w_size_h=4, w_stride_h=1, val_percentage={'train': 2.5/3, 'val':1/3, 'test':1/3}):\n",
    "    for split in splits:\n",
    "        # create records\n",
    "        dataset_list = create_dataset_list(root_dir, paths[split], w_size_h=w_size_h, w_stride_h=w_stride_h, val_percentage=val_percentage[split])\n",
    "        # create dataframe\n",
    "        dataset = pd.DataFrame(dataset_list)\n",
    "        if split != 'train':\n",
    "            # group by sample_id (data_file) and create a list of valid offsets\n",
    "            records = dataset.groupby('data_file').apply(lambda x: {\n",
    "                    'data_file' : x.data_file.iloc[0],\n",
    "                    'user_id' : x.user_id.iloc[0],\n",
    "                    'sample_id' : x.sample_id.iloc[0],\n",
    "                    'label' : x.label.iloc[0],\n",
    "                    'valid' : 1,\n",
    "                    'offsets' : _create_offsets(x),\n",
    "                })\n",
    "            dataset = pd.DataFrame().from_records(records.to_list())\n",
    "        dataset.to_csv(output_dir/f\"{split}_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"../../datasets/SPGC_challenge_track_2_release\")\n",
    "output_dir = Path(\"../data/track2/aggregated_volund\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "w_size_h = 5.334\n",
    "w_stride_h = 1\n",
    "val_percentage = {'train': 2.5/3, 'val':1/3, 'test':1/3}\n",
    "\n",
    "save_dataset(root_dir, output_dir, w_size_h=w_size_h, w_stride_h=w_stride_h, val_percentage=val_percentage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute per-subject statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 0.4846786305433228, 'std': 0.3840024955591947}\n"
     ]
    }
   ],
   "source": [
    "# Compute Stats\n",
    "root_dir = Path(\"../../datasets/SPGC_challenge_track_2_release/training_data\")\n",
    "stats = {}\n",
    "\n",
    "for user in os.listdir(root_dir):\n",
    "    user_id = int(user.split(\"_\")[1])\n",
    "    user_dir = root_dir/Path(f\"{user}/train/non-relapse\")\n",
    "    user_dfs = []\n",
    "    for sample in os.listdir(user_dir):\n",
    "        # read aggregated data\n",
    "        user_dfs.append(pd.read_csv(user_dir/Path(sample)/\"aggregated_data.csv\", index_col=0))\n",
    "    # concatenate dataframes\n",
    "    df = pd.concat(user_dfs).replace([np.inf, -np.inf], np.nan)\n",
    "    mean = df.mean()\n",
    "    std = df.std()\n",
    "    record = pd.concat([mean, std], axis=1)\n",
    "    record.columns = ['mean', 'std']\n",
    "    stats[user_id] = record.transpose().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 0.4846786305433228, 'std': 0.3840024955591947}\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path(\"../data/track2/\")\n",
    "\n",
    "#with open(output_dir/\"subject_stats.json\", \"w\") as f:\n",
    "#    json.dump(stats, f)\n",
    "\n",
    "with open(output_dir/\"subject_stats.json\", \"r\") as f:\n",
    "    stats = json.load(f)\n",
    "\n",
    "user = '9'\n",
    "print(stats[user]['mean_acc_norm'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|ââââââââââ| 2352/2352 [00:12<00:00, 194.24it/s]\n"
     ]
    }
   ],
   "source": [
    "class EPreventionDataset(CacheDataset):\n",
    "    def __init__(self, split_path, split, transforms, max_samples=None, subject=None, cache_num = sys.maxsize, cache_rate=1.0, num_workers=1):    \n",
    "        \n",
    "        self.split = split\n",
    "        self.max_samples = max_samples\n",
    "        \n",
    "        data = self._generate_data_list(split_path/f\"{split}_dataset.csv\", subject=subject)\n",
    "\n",
    "        super().__init__(data, transforms, cache_num=cache_num, cache_rate=cache_rate, num_workers=num_workers)\n",
    "        \n",
    "     \n",
    "    #split data in train, val and test sets in a reproducible way\n",
    "    def _generate_data_list(self, split_path, subject = None):\n",
    "\n",
    "        # open csv with observations\n",
    "        data_list = pd.read_csv(split_path, index_col=0, nrows=self.max_samples)\n",
    "        if subject is not None:\n",
    "           # filter subject\n",
    "            data_list = data_list[data_list['user_id']==subject]\n",
    "        # filter valid\n",
    "        data_list = data_list[data_list.valid.astype(bool)]\n",
    "        # save ditribution\n",
    "        count_distribution = data_list.label.value_counts().sort_index().to_numpy()\n",
    "        num_samples = len(data_list)\n",
    "        self.distribution = count_distribution / num_samples\n",
    "\n",
    "        return data_list.to_dict('records')  \n",
    "    \n",
    "    def get_label_proportions(self):\n",
    "\n",
    "        return self.distribution\n",
    "\n",
    "\n",
    "class AppendRootDirD(MapTransform):\n",
    "\n",
    "    def __init__(self, keys: KeysCollection, root_dir):\n",
    "        super().__init__(keys)\n",
    "        self.root_dir = root_dir\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d[k] = os.path.join(self.root_dir,d[k])\n",
    "        return d\n",
    "\n",
    "class LoadAggregatedDataD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, split):\n",
    "        super().__init__(keys)\n",
    "        self.split = split\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            if self.split == 'train':\n",
    "                d['data'] = pd.read_csv(d[k],\n",
    "                    skiprows=lambda x : x in range(1, d['start_data_row']+1),\n",
    "                    nrows=d['end_data_row']-d['start_data_row']\n",
    "                ) \n",
    "            else:\n",
    "                d['data'] = pd.read_csv(d[k])\n",
    "            del d[k]\n",
    "            del d['data']['Unnamed: 0']\n",
    "        if 'valid' in d.keys(): del d['valid']\n",
    "        if 'start_data_row' in d.keys(): del d['start_data_row']\n",
    "        if 'end_data_row' in d.keys(): del d['end_data_row']\n",
    "        return d\n",
    "\n",
    "class ImputeMedianD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection):\n",
    "        super().__init__(keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            # impute median\n",
    "            d[k] = d[k].replace([np.inf, -np.inf], np.nan)\n",
    "            d[k] = d[k].fillna(d[k].median())\n",
    "            # check whole nan cols\n",
    "            user = str(d['user_id'])\n",
    "            for col in d[k].columns:\n",
    "                if d[k][col].isna().all():\n",
    "                    d[k][col] = stats[user][col]['mean']\n",
    "        return d\n",
    "\n",
    "class ToNumpyD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection):\n",
    "        super().__init__(keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d[k] = d[k].to_numpy()\n",
    "        return d\n",
    "\n",
    "class StandardizeD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection):\n",
    "        super().__init__(keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            user = str(d['user_id'])\n",
    "            means = torch.tensor([stat['mean'] for _, stat in stats[user].items()])\n",
    "            stds = torch.tensor([stat['std'] for _, stat in stats[user].items()])\n",
    "            means[7:] = 0.\n",
    "            stds[7:] = 1.\n",
    "            #print(means, stds)\n",
    "            d[k] = (d[k] - means)/stds\n",
    "        return d\n",
    "\n",
    "class TransposeD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection):\n",
    "        super().__init__(keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d[k] = d[k].t()\n",
    "        return d\n",
    "\n",
    "class FlattenD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection):\n",
    "        super().__init__(keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            if len(d[k].shape) == 2:\n",
    "                d[k] = d[k].flatten()\n",
    "            else:\n",
    "                d[k] = d[k].flatten(start_dim=1)\n",
    "        return d\n",
    "\n",
    "root_dir = Path(\"../../datasets/SPGC_challenge_track_2_release\")\n",
    "\n",
    "transforms = [\n",
    "        ToTensorD(['label'],dtype=torch.long),\n",
    "        AppendRootDirD(['data_file'], root_dir),\n",
    "        LoadAggregatedDataD(['data_file'], 'train'),\n",
    "        ImputeMedianD(['data']),\n",
    "        ToNumpyD(['data']),\n",
    "        ToTensorD(['data'], dtype=torch.float),\n",
    "        StandardizeD(['data']),\n",
    "        TransposeD(['data']),\n",
    "        FlattenD(['data'])\n",
    "]\n",
    "\n",
    "transforms = Compose(transforms)\n",
    "\n",
    "train_data = EPreventionDataset(Path(\"../data/track2/\"), 'train', transforms=transforms, subject=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Transforms for Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|ââââââââââ| 544/544 [00:05<00:00, 103.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import ConstantPad1d, ReplicationPad1d\n",
    "\n",
    "class PadShortSequenceD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, output_size, padding, mode):\n",
    "        super().__init__(keys)\n",
    "        assert padding in ['replication', 'zero'], \"Select Proper Padding Mode: Allowed same and zero\"\n",
    "        assert mode in ['head', 'center', 'tail'], \"Select Proper Mode: Allowed head, center and tail\"\n",
    "        self.output_size = output_size\n",
    "        self.padding = padding\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        w_in = d['data'].shape[-1]\n",
    "        if w_in >= self.output_size:\n",
    "            return d\n",
    "        pad_size = self.output_size - w_in\n",
    "        if self.mode == 'head':\n",
    "            padding = (pad_size, 0)\n",
    "        elif self.mode == 'tail':\n",
    "            padding = (0, pad_size)\n",
    "        elif self.mode == 'center' and pad_size%2==0:\n",
    "            padding = pad_size//2\n",
    "        elif self.mode == 'center' and pad_size%2==1:\n",
    "            padding = (pad_size//2, pad_size//2+1)\n",
    "        pad_fn = self._get_pad_fn(padding)\n",
    "        for k in self.keys:\n",
    "            d[k] = pad_fn(d[k])\n",
    "        return d\n",
    "\n",
    "    def _get_pad_fn(self, padding):\n",
    "        return ConstantPad1d(padding, 0) if self.padding == 'zero' else ReplicationPad1d(padding)\n",
    "\n",
    "class CreateVotingBatchD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection):\n",
    "        super().__init__(keys)\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        offsets = eval(d['offsets'])\n",
    "        for k in self.keys:\n",
    "            windows = [d[k][:, start:stop].unsqueeze(0) for (start, stop) in offsets]\n",
    "            d[k] = torch.cat(windows, dim=0)\n",
    "        if 'offsets' in d.keys():\n",
    "            del d['offsets']\n",
    "        return d\n",
    "\n",
    "eval_transforms = [\n",
    "        ToTensorD(['label'],dtype=torch.long),\n",
    "        AppendRootDirD(['data_file'], root_dir),\n",
    "        LoadAggregatedDataD(['data_file'], 'val'),\n",
    "        ImputeMedianD(['data']),\n",
    "        ToNumpyD(['data']),\n",
    "        ToTensorD(['data'], dtype=torch.float),\n",
    "        StandardizeD(['data']),\n",
    "        TransposeD(['data']),\n",
    "        CreateVotingBatchD(['data']),\n",
    "        PadShortSequenceD(['data'], output_size=48, padding='replication', mode='head'),\n",
    "        FlattenD(['data'])\n",
    "]\n",
    "\n",
    "eval_transforms = Compose(eval_transforms)\n",
    "\n",
    "val_data = EPreventionDataset(Path(\"../data/track2/\"), 'test', transforms=eval_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 0,\n",
       " 'sample_id': 0,\n",
       " 'label': tensor(-1),\n",
       " 'data': tensor([[-0.1862, -0.3519, -0.3272,  ...,  0.0069,  0.0069,  0.0069],\n",
       "         [-0.3869, -0.3852, -0.3852,  ...,  0.0069,  0.0069,  0.0069],\n",
       "         [-0.3750, -0.3554, -0.3515,  ...,  0.0069,  0.0069,  0.0069],\n",
       "         ...,\n",
       "         [-0.1448,  1.2524,  0.8345,  ...,  0.0069,  0.0069,  0.0069],\n",
       "         [-0.1655,  1.0724,  2.1658,  ...,  0.0069,  0.0069,  0.0069],\n",
       "         [ 0.5302,  1.9109,  0.8617,  ...,  0.0069,  0.0069,  0.0069]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "000d80ed2087394a6b578f046794fbe46f974e9842e1e873401be47fee2626f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
