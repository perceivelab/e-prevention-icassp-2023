{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s_calcagno/miniconda3/envs/pytorch/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.impute import KNNImputer\n",
    "from monai.config import KeysCollection\n",
    "from torch.nn import ConstantPad1d, ReplicationPad1d\n",
    "from monai.transforms import Randomizable, MapTransform, Transform\n",
    "from monai.data import CacheDataset, DataLoader, DistributedSampler\n",
    "from torch.utils.data import RandomSampler, SequentialSampler, BatchSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_range = {\n",
    "    \"acc_X\" : (-19.6, 19.6),\n",
    "    \"acc_Y\" : (-19.6, 19.6),\n",
    "    \"acc_Z\" : (-19.6, 19.6),\n",
    "    \"gyr_X\" : (-573, 573),\n",
    "    \"gyr_Y\" : (-573, 573),\n",
    "    \"gyr_Z\" : (-573, 573),\n",
    "    \"heartRate\" : (0, 255),\n",
    "    \"rRInterval\" : (0, 2000),\n",
    "}\n",
    "\n",
    "def validate(window):\n",
    "    invalid_filter = pd.Series(False, window.index)\n",
    "    for col, (a, b) in valid_range.items():\n",
    "        invalid_filter = invalid_filter | (window[col]<a) | (window[col]>b)\n",
    "    return 1- (len(window[invalid_filter])/len(window))\n",
    "\n",
    "def get_observations(root_dir, user, split, sample, w_size_h, w_stride_h, val_percentage, drop_short_sequences=True):\n",
    "    obs = []\n",
    "    # open data file\n",
    "    if split == 'test':\n",
    "        data_file = Path('test_data')/Path(sample)/\"data.csv\"\n",
    "        step_file = Path('test_data')/Path(sample)/\"step.csv\"\n",
    "        # save user\n",
    "        user_id = -1 # to be predicted\n",
    "    else:\n",
    "        data_file = Path('training_data')/Path(user)/Path(split)/Path(sample)/\"data.csv\"\n",
    "        step_file = Path('training_data')/Path(user)/Path(split)/Path(sample)/\"step.csv\"\n",
    "        user_id = int(user.split(\"_\")[1])\n",
    "        \n",
    "    w_size = int(w_size_h*60*12)\n",
    "    w_stride = int(w_stride_h*60*12)\n",
    "    data = pd.read_csv(Path(root_dir)/data_file)\n",
    "    if len(data) < w_size:\n",
    "        if split == 'train' and drop_short_sequences:\n",
    "            return obs\n",
    "        # Consider short windows in validation and test\n",
    "        else:\n",
    "            validity = validate(data)\n",
    "            return [{\n",
    "                'data_file' : data_file,\n",
    "                'step_file' : step_file,\n",
    "                'label' : user_id,\n",
    "                'valid' : validity >= val_percentage,\n",
    "                'start_data_row' : 0,\n",
    "                'end_data_row' : len(data) \n",
    "            }]\n",
    "    \n",
    "    # slide windows\n",
    "    for start in range(0, len(data)-w_size, w_stride):\n",
    "        stop = start + w_size # excluded\n",
    "        window = data.loc[start:stop-1] # upperbound is included\n",
    "        # check validity\n",
    "        validity = validate(window)\n",
    "        obs.append({\n",
    "            'data_file' : data_file,\n",
    "            'step_file' : step_file,\n",
    "            'label' : user_id,\n",
    "            'valid' : validity >= val_percentage,\n",
    "            'start_data_row' : start,\n",
    "            'end_data_row' : stop \n",
    "        })\n",
    "    return obs\n",
    "    \n",
    "def create_dataset_list(root_dir, split='train', w_size_h=3, w_stride_h=3, val_percentage=2.5/3, drop_short_sequences=True):\n",
    "    dataset_list = []\n",
    "\n",
    "    if split == 'test':\n",
    "        # iterate over observations\n",
    "        for sample in tqdm(os.listdir(root_dir/'test_data')):\n",
    "            # open data file\n",
    "            obs = get_observations(root_dir, -1, split, sample, w_size_h, w_stride_h, val_percentage, drop_short_sequences)\n",
    "            dataset_list += obs\n",
    "    else:\n",
    "        # iterate over observations\n",
    "        for user in tqdm(os.listdir(root_dir/'training_data')):\n",
    "            # get user dir\n",
    "            split_dir = Path(root_dir)/Path('training_data')/Path(user)/Path(split)\n",
    "            for sample in tqdm(os.listdir(split_dir)):\n",
    "                # open data file\n",
    "                #sample_dir = Path(split_dir)/Path(sample)\n",
    "                obs = get_observations(root_dir, user, split, sample, w_size_h, w_stride_h, val_percentage, drop_short_sequences)\n",
    "                dataset_list += obs\n",
    "\n",
    "    return dataset_list\n",
    "\n",
    "def _create_offsets(x):\n",
    "    if len(x[x.valid]) == 0:\n",
    "        return list(zip(x.start_data_row, x.end_data_row))\n",
    "    return list(zip(x[x.valid].start_data_row, x[x.valid].end_data_row))\n",
    "\n",
    "def save_dataset(root_dir, output_dir, split='train', w_size_h=3, w_stride_h=3, val_percentage=2.5/3, drop_short_sequences=True):\n",
    "    dataset_list = create_dataset_list(root_dir, split, w_size_h, w_stride_h, val_percentage, drop_short_sequences)\n",
    "    # create dataframe\n",
    "    dataset = pd.DataFrame(dataset_list)\n",
    "    if split != 'train':\n",
    "        # group by sample_id (data_file) and create a list of valid offsets\n",
    "        records = dataset.groupby('data_file').apply(lambda x: {\n",
    "            'data_file': x.data_file.iloc[0],\n",
    "            'step_file': x.step_file.iloc[0],\n",
    "            'label' : x.label.iloc[0],\n",
    "            'valid' : True,\n",
    "            'offsets': _create_offsets(x),\n",
    "        # TODO: don't filter if the list is empty\n",
    "        })\n",
    "        dataset = pd.DataFrame().from_records(records.to_list())\n",
    "    dataset.to_csv(output_dir/f\"{split}_dataset.csv\")\n",
    "\n",
    "def compute_metrics():\n",
    "\n",
    "    # init wandb run\n",
    "    run = wandb.init()\n",
    "    split = 'train'\n",
    "    w_size_h = wandb.config.w_size_h\n",
    "    w_stride_h = wandb.config.w_stride_h\n",
    "    val_percentage = wandb.config.val_percentage\n",
    "\n",
    "    if w_stride_h > w_size_h:\n",
    "        wandb.log({\n",
    "            'w_size_h' : w_size_h,\n",
    "            'w_stride_h' : w_stride_h,\n",
    "            'val_percentage' : 0,\n",
    "            'min_count' : 0,\n",
    "            'l2' : 1000\n",
    "        })\n",
    "        return\n",
    "\n",
    "    # create dataset\n",
    "    dataset_list = create_dataset_list(root_dir, split, w_size_h, w_stride_h, val_percentage)\n",
    "    # create dataframe\n",
    "    dataset = pd.DataFrame(dataset_list)\n",
    "\n",
    "    # filter valid\n",
    "    dataset = dataset[dataset.valid]\n",
    "\n",
    "    # plot distribution\n",
    "    count_distribution = dataset.label.value_counts().sort_index().to_numpy()\n",
    "    min_count = min(count_distribution)\n",
    "    num_samples = sum(count_distribution)\n",
    "    distribution = count_distribution / num_samples\n",
    "    # compute uniform\n",
    "    uniform_probability = len(count_distribution / num_samples)\n",
    "    uniform = np.zeros(len(count_distribution)) + uniform_probability\n",
    "    l2 = np.linalg.norm(uniform - distribution)\n",
    "\n",
    "    wandb.log({\n",
    "        'w_size_h' : w_size_h,\n",
    "        'w_stride_h' : w_stride_h,\n",
    "        'val_percentage' : val_percentage,\n",
    "        'min_count' : min_count,\n",
    "        'l2' : l2\n",
    "    })\n",
    "\n",
    "def run_sweep():\n",
    "    # Define sweep config\n",
    "    sweep_configuration = {\n",
    "        'method': 'grid',\n",
    "        'name': 'distribution_sweep',\n",
    "        'metric': {'goal': 'minimize', 'name': 'l2'},\n",
    "        'parameters': \n",
    "        {\n",
    "            'w_size_h': {'values': [1, 3, 6, 9, 12, 15, 18]},\n",
    "            'w_stride_h': {'values': [1, 3, 6, 9]},\n",
    "            'val_percentage': {'values': [0.75, 2.5/3, 0.9]}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Initialize sweep by passing in config. (Optional) Provide a name of the project.\n",
    "    sweep_id = wandb.sweep(sweep=sweep_configuration, project='spgc_data_distribution')\n",
    "\n",
    "    # Start sweep job.\n",
    "    wandb.agent(sweep_id, function=compute_metrics, project='spgc_data_distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a279679453824d1cb8d5c8a07dd980c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# where do you want to place?\n",
    "output_dir = Path(\"../data/track1/\")\n",
    "# data dir\n",
    "root_dir = Path(\"../../datasets/SPGC_challenge_track_1_release\")\n",
    "\n",
    "split = 'val'\n",
    "w_size_h = 1.5\n",
    "w_stride_h = 1.5\n",
    "val_percentage = 1/3\n",
    "drop_short_sequences = True\n",
    "\n",
    "save_dataset(root_dir, output_dir, split, w_size_h, w_stride_h, val_percentage, drop_short_sequences)\n",
    "#run_sweep()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Folds for Cross Validation\n",
    "The general idea is to use validation set as a fold and then separate train into 4 separate folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dir\n",
    "root_dir = Path(\"../../datasets/SPGC_challenge_track_1_release\")\n",
    "\n",
    "for user in os.listdir(root_dir/Path(\"training_data\")):\n",
    "    train_samples = root_dir/Path(\"training_data\")/Path(user)/Path(\"train\")\n",
    "    val_samples = root_dir/Path(\"training_data\")/Path(user)/Path(\"val\")\n",
    "    train_samples = len(os.listdir(train_samples))\n",
    "    val_samples = len(os.listdir(val_samples))\n",
    "    total = train_samples+ val_samples\n",
    "    #print(train_samples/total, val_samples/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get paths\n",
    "\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "num_folds=4\n",
    "folds = {x : [] for x in range(num_folds+1)}\n",
    "\n",
    "for user in os.listdir(root_dir/Path(\"training_data\")):\n",
    "    train_samples = os.listdir(root_dir/Path(\"training_data\")/Path(user)/Path(\"train\"))\n",
    "    train_samples = list(map(lambda x: Path(\"training_data\")/Path(user)/Path(\"train\")/Path(x), train_samples))\n",
    "    #print(len(train_samples))\n",
    "    user_folds = list(split(train_samples, num_folds))\n",
    "    #print(sum([len(fold) for fold in user_folds]))\n",
    "    for i in range(num_folds):\n",
    "        folds[i].extend(user_folds[i])\n",
    "    val_samples = os.listdir(root_dir/Path(\"training_data\")/Path(user)/Path(\"val\"))\n",
    "    val_samples = list(map(lambda x: Path(\"training_data\")/Path(user)/Path(\"val\")/Path(x), val_samples))\n",
    "    folds[num_folds].extend(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593\n",
      "580\n",
      "573\n",
      "558\n",
      "495\n"
     ]
    }
   ],
   "source": [
    "for fold in folds:\n",
    "    print(len(folds[fold]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 'train', 37)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_path(path):\n",
    "    path = str(path)\n",
    "    _, user, split, id = path.split(\"/\")\n",
    "    user = int(user.split(\"_\")[1])\n",
    "    id = int(id)\n",
    "    return user, split, id\n",
    "\n",
    "parse_path(folds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_range = {\n",
    "    \"acc_X\" : (-19.6, 19.6),\n",
    "    \"acc_Y\" : (-19.6, 19.6),\n",
    "    \"acc_Z\" : (-19.6, 19.6),\n",
    "    \"gyr_X\" : (-573, 573),\n",
    "    \"gyr_Y\" : (-573, 573),\n",
    "    \"gyr_Z\" : (-573, 573),\n",
    "    \"heartRate\" : (0, 255),\n",
    "    \"rRInterval\" : (0, 2000),\n",
    "}\n",
    "\n",
    "def validate(window):\n",
    "    invalid_filter = pd.Series(False, window.index)\n",
    "    for col, (a, b) in valid_range.items():\n",
    "        invalid_filter = invalid_filter | (window[col]<a) | (window[col]>b)\n",
    "    return 1- (len(window[invalid_filter])/len(window))\n",
    "\n",
    "def get_observations(root_dir, sample_path, split, w_size_h, w_stride_h, val_percentage, drop_short_sequences=True):\n",
    "    obs = []\n",
    "    # open data file\n",
    "    data_file = Path(sample_path)/\"data.csv\"\n",
    "    step_file = Path(sample_path)/\"step.csv\"\n",
    "    user_id, _, id = parse_path(sample_path)\n",
    "  \n",
    "    w_size = int(w_size_h*60*12)\n",
    "    w_stride = int(w_stride_h*60*12)\n",
    "    data = pd.read_csv(Path(root_dir)/data_file)\n",
    "    \n",
    "    if len(data) < w_size:\n",
    "        if split == 'train' and drop_short_sequences:\n",
    "            return obs\n",
    "        # Consider short windows in validation and test\n",
    "        else:\n",
    "            validity = validate(data)\n",
    "            return [{\n",
    "                'data_file' : data_file,\n",
    "                'step_file' : step_file,\n",
    "                'label' : user_id,\n",
    "                'valid' : validity >= val_percentage,\n",
    "                'start_data_row' : 0,\n",
    "                'end_data_row' : len(data) \n",
    "            }]\n",
    "    \n",
    "    # slide windows\n",
    "    for start in range(0, len(data)-w_size, w_stride):\n",
    "        stop = start + w_size # excluded\n",
    "        window = data.loc[start:stop-1] # upperbound is included\n",
    "        # check validity\n",
    "        validity = validate(window)\n",
    "        obs.append({\n",
    "            'data_file' : data_file,\n",
    "            'step_file' : step_file,\n",
    "            'label' : user_id,\n",
    "            'valid' : validity >= val_percentage,\n",
    "            'start_data_row' : start,\n",
    "            'end_data_row' : stop \n",
    "        })\n",
    "\n",
    "    return obs\n",
    "    \n",
    "def create_fold(root_dir, folds, eval_fold_id=0, w_size_h=3, w_stride_h=3, val_percentage_train=2.5/3, val_percentage_eval=1/3, drop_short_sequences=True):\n",
    "\n",
    "    eval_samples = folds[eval_fold_id]\n",
    "    train_samples = []\n",
    "    for k in folds:\n",
    "        if k != eval_fold_id:\n",
    "            train_samples.extend(folds[k])\n",
    "\n",
    "    train_observations = []\n",
    "    # iterate over observations\n",
    "    for sample in tqdm(train_samples):\n",
    "        obs = get_observations(root_dir, sample, 'train', w_size_h, w_stride_h, val_percentage_train, drop_short_sequences)\n",
    "        train_observations += obs\n",
    "\n",
    "    eval_observations = []\n",
    "    # iterate over observations\n",
    "    for sample in tqdm(eval_samples):\n",
    "        obs = get_observations(root_dir, sample, 'val', w_size_h, w_stride_h, val_percentage_eval, drop_short_sequences)\n",
    "        eval_observations += obs\n",
    "\n",
    "    return train_observations, eval_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _create_offsets(x):\n",
    "    if len(x[x.valid]) == 0:\n",
    "        return list(zip(x.start_data_row, x.end_data_row))\n",
    "    return list(zip(x[x.valid].start_data_row, x[x.valid].end_data_row))\n",
    "\n",
    "def save_dataset(root_dir, output_dir, folds, eval_fold_id=0, w_size_h=3, w_stride_h=3, val_percentage_train=2.5/3, val_percentage_eval=1/3, drop_short_sequences=True):\n",
    "    train_obs, eval_obs = create_fold(root_dir, folds, eval_fold_id, w_size_h, w_stride_h, val_percentage_train, val_percentage_eval, drop_short_sequences)\n",
    "    # create train dataframe\n",
    "    dataset = pd.DataFrame(train_obs)\n",
    "    dataset.to_csv(output_dir/f\"fold{eval_fold_id}/train_dataset.csv\")\n",
    "\n",
    "    # create eval dataframe\n",
    "    dataset = pd.DataFrame(eval_obs)\n",
    "    # group by sample_id (data_file) and create a list of valid offsets\n",
    "    records = dataset.groupby('data_file').apply(lambda x: {\n",
    "        'data_file': x.data_file.iloc[0],\n",
    "        'step_file': x.step_file.iloc[0],\n",
    "        'label' : x.label.iloc[0],\n",
    "        'valid' : True,\n",
    "        'offsets': _create_offsets(x),\n",
    "    # TODO: don't filter if the list is empty\n",
    "    })\n",
    "    dataset = pd.DataFrame().from_records(records.to_list())\n",
    "    dataset.to_csv(output_dir/f\"fold{eval_fold_id}/val_dataset.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where do you want to place?\n",
    "output_dir = Path(\"../data/track1/\")\n",
    "save_dataset(root_dir, output_dir, folds, eval_fold_id=0)\n",
    "save_dataset(root_dir, output_dir, folds, eval_fold_id=1)\n",
    "save_dataset(root_dir, output_dir, folds, eval_fold_id=2)\n",
    "save_dataset(root_dir, output_dir, folds, eval_fold_id=3)\n",
    "save_dataset(root_dir, output_dir, folds, eval_fold_id=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPreventionDataset(CacheDataset):\n",
    "    def __init__(self, split_path, split, transforms, max_samples=None, cache_num = sys.maxsize, cache_rate=1.0, num_workers=1):    \n",
    "        \n",
    "        self.split = split\n",
    "        self.max_samples = max_samples\n",
    "        \n",
    "        data = self._generate_data_list(split_path/f\"{split}_dataset.csv\")\n",
    "\n",
    "        super().__init__(data, transforms, cache_num=cache_num, cache_rate=cache_rate, num_workers=num_workers)\n",
    "        \n",
    "     \n",
    "    #split data in train, val and test sets in a reproducible way\n",
    "    def _generate_data_list(self, split_path):\n",
    "\n",
    "        # open csv with observations\n",
    "        data_list = pd.read_csv(split_path, index_col=0, nrows=self.max_samples)\n",
    "        # filter valid\n",
    "        data_list = data_list[data_list.valid]\n",
    "        # save ditribution\n",
    "        count_distribution = data_list.label.value_counts().sort_index().to_numpy()\n",
    "        num_samples = len(data_list)\n",
    "        self.distribution = count_distribution / num_samples\n",
    "\n",
    "        return data_list.to_dict('records')  \n",
    "    \n",
    "    def get_label_proportions(self):\n",
    "\n",
    "        return self.distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 100/100 [00:00<00:00, 898137.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data_file': 'training_data/user_00/train/03/data.csv',\n",
       " 'step_file': 'training_data/user_00/train/03/step.csv',\n",
       " 'label': 0,\n",
       " 'valid': True,\n",
       " 'offsets': '[(0, 2160), (2160, 4320), (4320, 6480), (6480, 8640), (8640, 10800)]'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = EPreventionDataset(split_path=Path(\"../data/track1/fold0\"), split='val', transforms=None, max_samples=100)\n",
    "ds[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppendRootDirD(MapTransform):\n",
    "    def __init__(self, keys: KeysCollection, root_dir):\n",
    "        super().__init__(keys)\n",
    "        self.root_dir = root_dir\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d[k] = os.path.join(self.root_dir,d[k])\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, split, use_sleeping):\n",
    "        super().__init__(keys)\n",
    "        self.split = split\n",
    "        if use_sleeping:\n",
    "            self.cols = ['acc_X', 'acc_Y', 'acc_Z', 'gyr_X', 'gyr_Y', 'gyr_Z', 'heartRate', 'rRInterval', 'timecol', 'sleeping']\n",
    "        else:\n",
    "            self.cols = ['acc_X', 'acc_Y', 'acc_Z', 'gyr_X', 'gyr_Y', 'gyr_Z', 'heartRate', 'rRInterval', 'timecol']\n",
    "\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            if self.split == 'train':\n",
    "                d['data'] = pd.read_csv(data[k],\n",
    "                    skiprows=lambda x : x in range(1, data['start_data_row']+1),\n",
    "                    nrows=data['end_data_row']-data['start_data_row'],\n",
    "                    usecols=self.cols)\n",
    "            else:\n",
    "                d['data'] = pd.read_csv(data[k], usecols=self.cols)\n",
    "            del d[k] \n",
    "        if 'valid' in d.keys(): del d['valid']\n",
    "        if 'start_data_row' in d.keys(): del d['start_data_row']\n",
    "        if 'end_data_row' in d.keys(): del d['end_data_row']\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractTimeD(MapTransform):\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d['time'] = d[k].timecol.astype('datetime64[ns]')\n",
    "            d[k].drop('timecol', inplace=True, axis=1)\n",
    "        return d\n",
    "\n",
    "class DeleteTimeD(MapTransform):\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            del d[k]\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadStepD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, use_calories):\n",
    "        super().__init__(keys)\n",
    "        if use_calories:\n",
    "            self.cols = ['start_time', 'end_time', 'totalSteps', 'distance', 'calories']\n",
    "        else:\n",
    "            self.cols = ['start_time', 'end_time', 'totalSteps', 'distance']\n",
    "\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d['step'] = pd.read_csv(data[k],\n",
    "                usecols=self.cols)\n",
    "            del d[k] \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToSequenceD(MapTransform):\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            if 'calories' in d[k].columns:\n",
    "                vm, vs, c = self._create_step_sequences(d['time'], d[k])\n",
    "                d['step'] = np.stack([vm, vs, c], axis=0)\n",
    "            else:\n",
    "                vm, vs = self._create_step_sequences(d['time'], d[k])\n",
    "                d['step'] = np.stack([vm, vs], axis=0)\n",
    "        if 'time' in d.keys():\n",
    "            del d['time']\n",
    "        return d\n",
    "\n",
    "    def _create_step_sequences(self, time, step):\n",
    "    \n",
    "        # create empty velocity vectors\n",
    "        velocity_m = np.zeros(len(time))\n",
    "        velocity_s = np.zeros(len(time))\n",
    "        if 'calories' in step.columns:\n",
    "            calories = np.zeros(len(time))\n",
    "\n",
    "        # add a column of period\n",
    "        step['start_time'] = pd.to_datetime(step['start_time'])\n",
    "        step['end_time'] = pd.to_datetime(step['end_time'])\n",
    "        step['period'] = step['end_time']-step['start_time']\n",
    "\n",
    "        for _, s in step.iterrows():\n",
    "            # get the period index in time array\n",
    "            idx = np.where((time > s.start_time) & (time < s.end_time))[0]\n",
    "            if len(idx) != 0:\n",
    "                # assign velocity in m/s in the period\n",
    "                velocity_m[idx] = s.distance / s.period.seconds\n",
    "                # assign velocity in steps/5s in the period\n",
    "                velocity_s[idx] = s.totalSteps / len(idx)\n",
    "                if 'calories' in step.columns:\n",
    "                    # assign calories in 5s\n",
    "                    calories[idx] = s.calories / len(idx)\n",
    "        if 'calories' in step.columns:\n",
    "            return velocity_m, velocity_s, calories\n",
    "        return velocity_m, velocity_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToArrayD(MapTransform):\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d[k] = d[k].to_numpy().transpose()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ranges = {\n",
    "    \"acc_X\" : (-19.6, 19.6),\n",
    "    \"acc_Y\" : (-19.6, 19.6),\n",
    "    \"acc_Z\" : (-19.6, 19.6),\n",
    "    \"gyr_X\" : (-573, 573),\n",
    "    \"gyr_Y\" : (-573, 573),\n",
    "    \"gyr_Z\" : (-573, 573),\n",
    "    \"heartRate\" : (0, 255),\n",
    "    \"rRInterval\" : (0, 2000),\n",
    "    \"sleeping\" : (0, 1)\n",
    "}\n",
    "\n",
    "class NormalizeDataD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, valid_ranges, use_sleeping):\n",
    "        super().__init__(keys)\n",
    "        self.valid_ranges = valid_ranges\n",
    "        vr_keys = list(valid_ranges.keys())\n",
    "        if not use_sleeping:\n",
    "            vr_keys.remove('sleeping')\n",
    "        self.min = np.array([valid_ranges[k][0] for k in vr_keys])\n",
    "        self.max = np.array([valid_ranges[k][1] for k in vr_keys])\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d[k] = ((d[k].transpose() - self.min)/(self.max - self.min)).transpose()\n",
    "        return d\n",
    "\n",
    "class NormalizeStepD(MapTransform):\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            mn = d[k].min(axis=1)\n",
    "            mx = d[k].max(axis=1)\n",
    "            r = mx - mn\n",
    "            r[np.where(r==0)] = 1\n",
    "            d[k] = ((d[k].transpose() - mn) / r).transpose()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolateDataD(MapTransform):\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d[k] = self._impute_invalid_values(d[k])\n",
    "        return d\n",
    "\n",
    "\n",
    "    def _impute_invalid_values(self, signals):\n",
    "        \n",
    "        # save input\n",
    "        input_signals = copy.deepcopy(signals)\n",
    "\n",
    "        # set a treshold for detect artifacts\n",
    "        signals[np.where(signals<0)] = -1.\n",
    "        signals[np.where(signals>1)] = -1.\n",
    "        \n",
    "        # interpolate\n",
    "        imputer = KNNImputer(missing_values=-1., n_neighbors=5, weights=\"distance\")\n",
    "        signals = imputer.fit_transform(signals)\n",
    "\n",
    "        # Preserve the dimensionality of short invalid sequences\n",
    "        if signals.shape[0] == 0:\n",
    "            return input_signals\n",
    "            \n",
    "        return signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenateStepD(MapTransform):\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d[k] = np.concatenate([d[k], d['step']], axis=0)\n",
    "        if 'step' in d.keys():\n",
    "            del d['step']\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadShortSequenceD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, output_size, padding, mode):\n",
    "        super().__init__(keys)\n",
    "        assert padding in ['replication', 'zero'], \"Select Proper Padding Mode: Allowed same and zero\"\n",
    "        assert mode in ['head', 'center', 'tail'], \"Select Proper Mode: Allowed head, center and tail\"\n",
    "        self.output_size = output_size\n",
    "        self.padding = padding\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        w_in = d['data'].shape[-1]\n",
    "        if w_in >= self.output_size:\n",
    "            return d\n",
    "        pad_size = self.output_size - w_in\n",
    "        if self.mode == 'head':\n",
    "            padding = (pad_size, 0)\n",
    "        elif self.mode == 'tail':\n",
    "            padding = (0, pad_size)\n",
    "        elif self.mode == 'center' and pad_size%2==0:\n",
    "            padding = pad_size//2\n",
    "        elif self.mode == 'center' and pad_size%2==1:\n",
    "            padding = (pad_size//2, pad_size//2+1)\n",
    "        pad_fn = self._get_pad_fn(padding)\n",
    "        for k in self.keys:\n",
    "            d[k] = pad_fn(d[k])\n",
    "        return d\n",
    "\n",
    "    def _get_pad_fn(self, padding):\n",
    "        return ConstantPad1d(padding, 0) if self.padding == 'zero' else ReplicationPad1d(padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateVotingBatchD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection):\n",
    "        super().__init__(keys)\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        offsets = eval(d['offsets'])\n",
    "        for k in self.keys:\n",
    "            windows = [d[k][:, start:stop].unsqueeze(0) for (start, stop) in offsets]\n",
    "            d[k] = torch.cat(windows, dim=0)\n",
    "        if 'offsets' in d.keys():\n",
    "            del d['offsets']\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import Compose, ToTensorD\n",
    "\n",
    "#from transforms import AppendRootDirD\n",
    "class AppendRootDirD(MapTransform):\n",
    "\n",
    "    def __init__(self, keys: KeysCollection, root_dir):\n",
    "        super().__init__(keys)\n",
    "        self.root_dir = root_dir\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d[k] = os.path.join(self.root_dir,d[k])\n",
    "        return d\n",
    "\n",
    "root_dir = Path(\"../../datasets/SPGC_challenge_track_1_release/\")\n",
    "split = 'val'\n",
    "w_size_h = 3\n",
    "w_stride_h = 3\n",
    "\n",
    "transforms = [\n",
    "        ToTensorD(['label'],dtype=torch.long),\n",
    "        AppendRootDirD(['data_file', 'step_file'], root_dir),\n",
    "        LoadDataD(['data_file'], split=split, use_sleeping=True),\n",
    "        ExtractTimeD(['data']),\n",
    "        LoadStepD(['step_file'], use_calories=True),\n",
    "        ConvertToSequenceD(['step']),\n",
    "        ToArrayD(['data']),\n",
    "        NormalizeDataD(['data'], valid_ranges=valid_ranges, use_sleeping=True),\n",
    "        InterpolateDataD(['data']),\n",
    "        NormalizeStepD(['step']),\n",
    "        ConcatenateStepD(['data']),\n",
    "        ToTensorD(['data'], dtype=torch.float),\n",
    "        CreateVotingBatchD(['data']),\n",
    "        PadShortSequenceD(['data'], output_size=w_size_h*60*12, padding='replication', mode='center')\n",
    "]\n",
    "\n",
    "transforms = Compose(transforms)\n",
    "ds = EPreventionDataset(split_path=Path(\"../data/track1/fold0\"), split='val', transforms=transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12, 2160])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]['data'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(args):\n",
    "    if not os.path.isdir(args.root_dir):\n",
    "        raise ValueError(\"Root directory root_dir must be a directory.\")\n",
    "    \n",
    "    basics_1 = [\n",
    "            ToTensorD(['label'],dtype=torch.long),\n",
    "            AppendRootDirD(['data_file', 'step_file'], args.root_dir)\n",
    "    ]\n",
    "\n",
    "    train_load = [LoadDataD(['data_file'], split='train', use_sleeping=args.use_sleeping)]\n",
    "    val_load = [LoadDataD(['data_file'], split='val', use_sleeping=args.use_sleeping)]\n",
    "\n",
    "    basics_2 = [       \n",
    "            ExtractTimeD(['data']),\n",
    "            ToArrayD(['data']),\n",
    "            NormalizeDataD(['data'], valid_ranges=args.valid_ranges, use_sleeping=args.use_sleeping),\n",
    "            InterpolateDataD(['data']),\n",
    "    ]\n",
    "\n",
    "    if args.use_steps:\n",
    "        basics_2 = [\n",
    "                *basics_2,\n",
    "                LoadStepD(['step_file'], use_calories=args.use_calories),\n",
    "                ConvertToSequenceD(['step']),\n",
    "                NormalizeStepD(['step']),\n",
    "                ConcatenateStepD(['data']),\n",
    "            ]\n",
    "    else:\n",
    "        basics_2 = [\n",
    "                *basics_2,\n",
    "                DeleteTimeD(['time'])\n",
    "            ]\n",
    "        \n",
    "    basics_2 = [\n",
    "                *basics_2,\n",
    "                ToTensorD(['data'], dtype=torch.float)\n",
    "            ]        \n",
    "\n",
    "    train_transforms = [\n",
    "        *basics_1,\n",
    "        *train_load,\n",
    "        *basics_2\n",
    "    ]\n",
    "\n",
    "    val_transforms = [\n",
    "        *basics_1,\n",
    "        *val_load,\n",
    "        *basics_2,\n",
    "        CreateVotingBatchD(['data']),\n",
    "        PadShortSequenceD(['data'], output_size=args.window_size, padding=args.padding_mode, mode=args.padding_loc)\n",
    "    ]\n",
    "\n",
    "    train_transforms = Compose(train_transforms)\n",
    "    val_transforms = Compose(val_transforms)\n",
    "    \n",
    "    dataset = {}\n",
    "    dataset[\"train\"] = EPreventionDataset(split_path=args.split_path, split='train', transforms=train_transforms, max_samples=args.max_samples, cache_rate=args.cache_rate)\n",
    "    dataset[\"val\"] = EPreventionDataset(split_path=args.split_path, split='val', transforms=val_transforms, max_samples=args.max_samples, cache_rate=args.cache_rate)\n",
    "    dataset[\"test\"] = EPreventionDataset(split_path=args.split_path, split='test', transforms=val_transforms, max_samples=args.max_samples, cache_rate=args.cache_rate)\n",
    "    \n",
    "    samplers = {}\n",
    "    if args.distributed:\n",
    "        samplers[\"train\"] = DistributedSampler(dataset[\"train\"], shuffle=True)\n",
    "        samplers[\"val\"] = DistributedSampler(dataset[\"val\"], shuffle=False)\n",
    "        samplers[\"test\"] = DistributedSampler(dataset[\"test\"], shuffle=False)\n",
    "    else:\n",
    "        samplers[\"train\"] = RandomSampler(dataset[\"train\"])\n",
    "        samplers[\"val\"] = SequentialSampler(dataset[\"val\"])\n",
    "        samplers[\"test\"] = SequentialSampler(dataset[\"test\"])\n",
    "\n",
    "    batch_sampler = {}\n",
    "    batch_sampler['train'] = BatchSampler(samplers[\"train\"], args.batch_size, drop_last=True)\n",
    "    batch_sampler['val'] = BatchSampler(samplers[\"val\"], 1, drop_last=False)\n",
    "    batch_sampler['test'] = BatchSampler(samplers[\"test\"], 1, drop_last=False)\n",
    "\n",
    "    loaders = {}\n",
    "    loaders[\"train\"] = DataLoader(dataset[\"train\"], batch_sampler = batch_sampler['train'], num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "    loaders[\"val\"] = DataLoader(dataset[\"val\"], batch_sampler = batch_sampler['val'], num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "    loaders[\"test\"] = DataLoader(dataset[\"test\"], batch_sampler = batch_sampler['test'], num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "    loss_weights = torch.Tensor(dataset[\"train\"].get_label_proportions())\n",
    "\n",
    "    return loaders, samplers, loss_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args={}\n",
    "\n",
    "args['use_sleeping'] = True\n",
    "args['use_steps'] = True\n",
    "args['use_calories'] = True\n",
    "args['valid_ranges'] = valid_ranges\n",
    "args['window_size'] = 3*60*12\n",
    "args['padding_mode'] = 'replication' # or 'zero'\n",
    "args['padding_loc'] = 'center' # or 'head' or 'tail'\n",
    "\n",
    "args['root_dir'] = Path(\"../../datasets/SPGC_challenge_track_1_release\") # where data are placed\n",
    "args['split_path'] = Path(\"../data/track1/fold0\")\n",
    "\n",
    "args['max_samples'] = 20\n",
    "args['cache_rate'] = 1.0\n",
    "\n",
    "args['distributed'] = False\n",
    "args['batch_size'] = 8\n",
    "\n",
    "args = argparse.Namespace(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 20/20 [00:01<00:00, 13.15it/s]\n",
      "Loading dataset: 100%|██████████| 20/20 [00:09<00:00,  2.09it/s]\n",
      "Loading dataset: 100%|██████████| 20/20 [00:10<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "loaders, samples, w_loss = get_loader(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c858351d824c77b8b4bcafc9478957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 12, 2160])\n",
      "torch.Size([1, 6, 12, 2160])\n",
      "torch.Size([1, 3, 12, 2160])\n",
      "torch.Size([1, 3, 12, 2160])\n",
      "torch.Size([1, 5, 12, 2160])\n",
      "torch.Size([1, 6, 12, 2160])\n",
      "torch.Size([1, 5, 12, 2160])\n",
      "torch.Size([1, 3, 12, 2160])\n",
      "torch.Size([1, 6, 12, 2160])\n",
      "torch.Size([1, 3, 12, 2160])\n",
      "torch.Size([1, 4, 12, 2160])\n",
      "torch.Size([1, 6, 12, 2160])\n",
      "torch.Size([1, 6, 12, 2160])\n",
      "torch.Size([1, 3, 12, 2160])\n",
      "torch.Size([1, 3, 12, 2160])\n",
      "torch.Size([1, 4, 12, 2160])\n",
      "torch.Size([1, 6, 12, 2160])\n",
      "torch.Size([1, 4, 12, 2160])\n",
      "torch.Size([1, 6, 12, 2160])\n",
      "torch.Size([1, 7, 12, 2160])\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(loaders['val']):\n",
    "    print(batch['data'].size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "000d80ed2087394a6b578f046794fbe46f974e9842e1e873401be47fee2626f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
