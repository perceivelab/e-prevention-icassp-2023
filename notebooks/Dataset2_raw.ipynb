{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track 2 (raw data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s_calcagno/miniconda3/envs/pytorch/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import pyhrv\n",
    "import scipy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from monai.config import KeysCollection\n",
    "from monai.transforms import MapTransform\n",
    "from monai.transforms import Compose, ToTensorD\n",
    "from monai.data import CacheDataset, DataLoader, DistributedSampler\n",
    "\n",
    "valid_range = {\n",
    "    \"acc_X\" : (-19.6, 19.6),\n",
    "    \"acc_Y\" : (-19.6, 19.6),\n",
    "    \"acc_Z\" : (-19.6, 19.6),\n",
    "    \"gyr_X\" : (-573, 573),\n",
    "    \"gyr_Y\" : (-573, 573),\n",
    "    \"gyr_Z\" : (-573, 573),\n",
    "    \"heartRate\" : (0, 255),\n",
    "    \"rRInterval\" : (0, 2000),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"../../datasets/SPGC_challenge_track_2_release/\")\n",
    "\n",
    "def get_paths(root_dir, split):\n",
    "    paths = []\n",
    "    if split != 'test':\n",
    "        base_dir = Path('training_data')\n",
    "        for user in os.listdir(root_dir/base_dir):\n",
    "            user_dir = base_dir/Path(user)/Path(split)\n",
    "            for status in os.listdir(root_dir/user_dir):\n",
    "                status_dir = user_dir/Path(status)\n",
    "                for sample in os.listdir(root_dir/status_dir):\n",
    "                    paths.append(status_dir/Path(sample))\n",
    "    else:\n",
    "        base_dir = Path('test_data')\n",
    "        for user in os.listdir(root_dir/base_dir):\n",
    "            user_dir = base_dir/Path(user)/Path(split)\n",
    "            for sample in os.listdir(root_dir/user_dir):\n",
    "                paths.append(user_dir/Path(sample))\n",
    "    return paths\n",
    "\n",
    "def parse_path(path):\n",
    "    path = str(path).split(\"/\")\n",
    "    user = int(path[1].split(\"_\")[1])\n",
    "    split = path[2]\n",
    "    if len(path)==5:\n",
    "        status = 1 if path[3]=='relapse' else 0\n",
    "        id = int(path[4])\n",
    "    else:\n",
    "        status = -1\n",
    "        id = int(path[3])\n",
    "    return user, split, status, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {split: get_paths(root_dir, split) for split in ['train', 'val', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: user_02, Status: non-relapse, Samples:25\n",
      "User: user_02, Status: relapse, Samples:13\n",
      "User: user_08, Status: non-relapse, Samples:13\n",
      "User: user_08, Status: relapse, Samples:3\n",
      "User: user_00, Status: non-relapse, Samples:31\n",
      "User: user_00, Status: relapse, Samples:9\n",
      "User: user_01, Status: non-relapse, Samples:22\n",
      "User: user_01, Status: relapse, Samples:57\n",
      "User: user_09, Status: non-relapse, Samples:21\n",
      "User: user_09, Status: relapse, Samples:73\n",
      "User: user_05, Status: non-relapse, Samples:27\n",
      "User: user_05, Status: relapse, Samples:22\n",
      "User: user_07, Status: non-relapse, Samples:29\n",
      "User: user_07, Status: relapse, Samples:93\n",
      "User: user_06, Status: non-relapse, Samples:26\n",
      "User: user_06, Status: relapse, Samples:4\n",
      "User: user_04, Status: non-relapse, Samples:22\n",
      "User: user_04, Status: relapse, Samples:3\n",
      "User: user_03, Status: non-relapse, Samples:21\n",
      "User: user_03, Status: relapse, Samples:17\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path('training_data')\n",
    "split = 'val'\n",
    "for user in os.listdir(root_dir/base_dir):\n",
    "    user_dir = base_dir/Path(user)/Path(split)\n",
    "    for status in os.listdir(root_dir/user_dir):\n",
    "        print(f\"User: {user}, Status: {status}, Samples:{len(os.listdir(root_dir/user_dir/Path(status)))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"../../datasets/SPGC_challenge_track_2_release/\")\n",
    "splits = ['train', 'val', 'test']\n",
    "paths = {split: get_paths(root_dir, split) for split in splits}\n",
    "\n",
    "def validate(window):\n",
    "    invalid_filter = window.isna().any(axis=1)\n",
    "    return 1- (len(window[invalid_filter])/len(window)) \n",
    "\n",
    "def get_observations(root_dir, path, w_size_h=4, w_stride_h=1, val_percentage=0.25):\n",
    "    \n",
    "    data = pd.read_csv(root_dir/path/\"data.csv\")\n",
    "    user, split, status, id = parse_path(path)\n",
    "    w_size = int(w_size_h*12*60)\n",
    "    w_stride = int(w_stride_h*12*60)\n",
    "    obs = []\n",
    "    path = Path(path/\"data.csv\")\n",
    "    # Treat short sequences\n",
    "    if len(data) < w_size:\n",
    "        if split == 'train':\n",
    "            return obs\n",
    "        # Consider short windows in validation and test\n",
    "        else:\n",
    "            validity = validate(data)\n",
    "            return [{\n",
    "                'data_file' : path,\n",
    "                'user_id' : user,\n",
    "                'sample_id' : id,\n",
    "                'label' : status,\n",
    "                'valid' : validity >= val_percentage,\n",
    "                'start_data_row' : 0,\n",
    "                'end_data_row' : len(data) \n",
    "            }]\n",
    "    \n",
    "    # Slide windows\n",
    "    for start in range(0, len(data)-w_size, w_stride):\n",
    "        stop = start + w_size # excluded\n",
    "        window = data.loc[start:stop-1] # upperbound is included\n",
    "        # check validity\n",
    "        validity = validate(window)\n",
    "        obs.append({\n",
    "            'data_file' : path,\n",
    "            'user_id' : user,\n",
    "            'sample_id' : id,\n",
    "            'label' : status,\n",
    "            'valid' : validity >= val_percentage,\n",
    "            'start_data_row' : start,\n",
    "            'end_data_row' :stop\n",
    "        })\n",
    "\n",
    "    return obs\n",
    "\n",
    "def create_dataset_list(root_dir, paths,  w_size_h=4, w_stride_h=1, val_percentage=0.25):\n",
    "    dataset_list = []\n",
    "    for sample in paths:\n",
    "        # open file\n",
    "        dataset_list.extend(get_observations(root_dir, sample, w_size_h=w_size_h, w_stride_h=w_stride_h, val_percentage=val_percentage))\n",
    "    return dataset_list\n",
    "\n",
    "def _create_offsets(x):\n",
    "    if len(x[x.valid]) == 0:\n",
    "        return list(zip(x.start_data_row, x.end_data_row))\n",
    "    return list(zip(x[x.valid].start_data_row, x[x.valid].end_data_row))\n",
    "\n",
    "def save_dataset(root_dir, output_dir, w_size_h=4, w_stride_h=1, val_percentage={'train': 2.5/3, 'val':1/3, 'test':1/3}):\n",
    "    for split in splits:\n",
    "        # create records\n",
    "        dataset_list = create_dataset_list(root_dir, paths[split], w_size_h=w_size_h, w_stride_h=w_stride_h, val_percentage=val_percentage[split])\n",
    "        # create dataframe\n",
    "        dataset = pd.DataFrame(dataset_list)\n",
    "        if split != 'train':\n",
    "            # group by sample_id (data_file) and create a list of valid offsets\n",
    "            records = dataset.groupby('data_file').apply(lambda x: {\n",
    "                    'data_file' : x.data_file.iloc[0],\n",
    "                    'user_id' : x.user_id.iloc[0],\n",
    "                    'sample_id' : x.sample_id.iloc[0],\n",
    "                    'label' : x.label.iloc[0],\n",
    "                    'valid' : 1,\n",
    "                    'offsets' : _create_offsets(x),\n",
    "                })\n",
    "            dataset = pd.DataFrame().from_records(records.to_list())\n",
    "        dataset.to_csv(output_dir/f\"{split}_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"../../datasets/SPGC_challenge_track_2_release\")\n",
    "output_dir = Path(\"../data/track2/raw_volund\")\n",
    "\n",
    "w_size_h = 2.8445\n",
    "w_stride_h = 2.8445\n",
    "val_percentage = {'train': 2.5/3, 'val':1/3, 'test':1/3}\n",
    "\n",
    "save_dataset(root_dir, output_dir, w_size_h=w_size_h, w_stride_h=w_stride_h, val_percentage=val_percentage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute per-subject Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64618a884044f858e58b2f8e4af54b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d8417a58c34b0089247d89b47d281b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_X': {'mean': -0.07740479162575463, 'std': 0.5343033072045137}, 'acc_Y': {'mean': -0.04565059638375774, 'std': 0.4632659545956309}, 'acc_Z': {'mean': 0.02541218994184653, 'std': 1.017124106303795}, 'gyr_X': {'mean': 0.04991267045793881, 'std': 5.508729026718897}, 'gyr_Y': {'mean': 0.19329504866729633, 'std': 5.835455661815294}, 'gyr_Z': {'mean': 0.02172364578913043, 'std': 7.335775080155805}, 'heartRate': {'mean': 76.91158360006465, 'std': 37.287505305403634}, 'rRInterval': {'mean': 719.9911809838902, 'std': 205.81339896547001}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da12f00a3c404c4f88d8055be96cb438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_X': {'mean': -0.0850953928717194, 'std': 0.3949884243991635}, 'acc_Y': {'mean': 0.016234903817932095, 'std': 0.49339803718533104}, 'acc_Z': {'mean': 0.0020008037716371238, 'std': 0.2982161607613874}, 'gyr_X': {'mean': 0.013499659163641242, 'std': 4.992049295122978}, 'gyr_Y': {'mean': 0.05001058881494166, 'std': 4.326428229719058}, 'gyr_Z': {'mean': -0.07173644678538918, 'std': 4.009702150871599}, 'heartRate': {'mean': 51.14093382088202, 'std': 43.983636478762236}, 'rRInterval': {'mean': 641.4522993438326, 'std': 385.5320666573568}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5780b6fbb40c47d782f4d5404357808c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_X': {'mean': -0.048440933939075295, 'std': 9.55237469473406}, 'acc_Y': {'mean': 0.07818054431582433, 'std': 0.4179387651682867}, 'acc_Z': {'mean': -0.05413172496386478, 'std': 0.4755626480131258}, 'gyr_X': {'mean': -0.022104286782690885, 'std': 6.422744574550959}, 'gyr_Y': {'mean': 0.116121176780356, 'std': 5.249311763286557}, 'gyr_Z': {'mean': -0.009221273915754976, 'std': 5.362507778972771}, 'heartRate': {'mean': 80.50294353467253, 'std': 39.6601647168719}, 'rRInterval': {'mean': 687.1113597150423, 'std': 218.96576478907238}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d516ab4bb9c42489a6d12241a22784d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_X': {'mean': -0.10997362242580694, 'std': 0.7058907120698802}, 'acc_Y': {'mean': 0.09147887657910124, 'std': 0.5197327561841306}, 'acc_Z': {'mean': -0.10899774189309082, 'std': 0.4518665285638489}, 'gyr_X': {'mean': 0.08776794049723474, 'std': 7.539833390757674}, 'gyr_Y': {'mean': 0.10356399750790797, 'std': 5.585229424425513}, 'gyr_Z': {'mean': -0.03976771107317502, 'std': 6.321834138993106}, 'heartRate': {'mean': 82.6555533299491, 'std': 30.416062624724187}, 'rRInterval': {'mean': 840.3548580496051, 'std': 219.2047892267108}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e4580dc911460cb922dd9adadd90bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_X': {'mean': -0.14136787738247256, 'std': 0.5426113879587267}, 'acc_Y': {'mean': 0.023768131889762057, 'std': 0.4689673054625551}, 'acc_Z': {'mean': -0.012439029132779127, 'std': 0.4406461071283258}, 'gyr_X': {'mean': -0.03419923962560961, 'std': 8.348854037356908}, 'gyr_Y': {'mean': 0.29665035106841764, 'std': 6.749337848374998}, 'gyr_Z': {'mean': 0.04557008292909999, 'std': 6.504126240479325}, 'heartRate': {'mean': 88.81403669694498, 'std': 21.499448769234718}, 'rRInterval': {'mean': 713.7407634794719, 'std': 158.07935502391476}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7bb7542fd846608aff59e80129017d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_X': {'mean': -0.10273361268933424, 'std': 0.6690189783109256}, 'acc_Y': {'mean': -0.004094247613722982, 'std': 0.42740236057568937}, 'acc_Z': {'mean': 0.05233723099241764, 'std': 3.1957801219838227}, 'gyr_X': {'mean': 0.13855119924909468, 'std': 5.481149317239829}, 'gyr_Y': {'mean': -0.1075337870131346, 'std': 3.981962705961871}, 'gyr_Z': {'mean': 0.05803836175868217, 'std': 57.86694218699615}, 'heartRate': {'mean': 73.2514684077721, 'std': 21.377218454198236}, 'rRInterval': {'mean': 907.002603854491, 'std': 186.34437718637585}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a98e393de04fe0a7536ba554c50145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_X': {'mean': -0.09319438383430002, 'std': 1.0549163715363512}, 'acc_Y': {'mean': -0.1318285128188079, 'std': 0.7824211155774202}, 'acc_Z': {'mean': 0.0581258162208312, 'std': 2.8133538480466758}, 'gyr_X': {'mean': 2.2043800008858794, 'std': 92.648168681159}, 'gyr_Y': {'mean': -0.47524791633864766, 'std': 12.29713208371014}, 'gyr_Z': {'mean': 0.04163716593449852, 'std': 89.8694566892467}, 'heartRate': {'mean': 72.3729479198667, 'std': 20.268623292508792}, 'rRInterval': {'mean': 861.8180264035229, 'std': 191.6807804814167}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364a755be83a41e5a44552ad99cd04cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_X': {'mean': -0.02092114265507649, 'std': 0.4154525036337986}, 'acc_Y': {'mean': -0.014012288149161977, 'std': 0.3781170387917316}, 'acc_Z': {'mean': 0.053518889409039976, 'std': 0.30976595106201094}, 'gyr_X': {'mean': 0.06170811909847024, 'std': 4.959383620928154}, 'gyr_Y': {'mean': 0.05460819371298155, 'std': 4.12906968431291}, 'gyr_Z': {'mean': 0.06711260025394976, 'std': 4.114395839546679}, 'heartRate': {'mean': 72.66138217353134, 'std': 29.686907125119014}, 'rRInterval': {'mean': 829.1710687354415, 'std': 268.86139018980003}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2569397987e4b65a49094fe96a4a0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_X': {'mean': -0.10977938591748378, 'std': 0.5849905651281385}, 'acc_Y': {'mean': 0.09671424934674658, 'std': 0.42242881138851734}, 'acc_Z': {'mean': 0.01227884443702697, 'std': 0.39596732436439214}, 'gyr_X': {'mean': 0.05048087384967409, 'std': 6.405825547399702}, 'gyr_Y': {'mean': 0.026655853000925794, 'std': 5.303059696431313}, 'gyr_Z': {'mean': -0.016025140677655692, 'std': 5.184809574545775}, 'heartRate': {'mean': 77.04926626391975, 'std': 27.502978136510016}, 'rRInterval': {'mean': 890.2228859012833, 'std': 214.60999275381027}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad8c0bbcd7540858a6b5c48e399102b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_X': {'mean': -0.22596949728009436, 'std': 1.4858015668008786}, 'acc_Y': {'mean': -0.06796499415827657, 'std': 4.585748126660567}, 'acc_Z': {'mean': 0.07151734662930911, 'std': 0.3360983510393319}, 'gyr_X': {'mean': 0.03623044481526213, 'std': 5.547699728914393}, 'gyr_Y': {'mean': -0.13753716699566793, 'std': 4.168085396206269}, 'gyr_Z': {'mean': 0.12295767572916634, 'std': 7.9929711279947835}, 'heartRate': {'mean': 85.51817947300428, 'std': 24.432683891364164}, 'rRInterval': {'mean': 784.9783992496891, 'std': 144.53229810610463}}\n"
     ]
    }
   ],
   "source": [
    "# Compute Stats\n",
    "root_dir = Path(\"../../datasets/SPGC_challenge_track_2_release/training_data\")\n",
    "stats = {}\n",
    "\n",
    "for user in tqdm(os.listdir(root_dir)):\n",
    "    user_id = int(user.split(\"_\")[1])\n",
    "    user_dir = root_dir/Path(f\"{user}/train/non-relapse\")\n",
    "    arrays = []\n",
    "    for sample in tqdm(os.listdir(user_dir)):\n",
    "        # read data\n",
    "        df = pd.read_csv(user_dir/Path(sample)/\"data.csv\")\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "        #print(df.columns)\n",
    "        arrays.append(df.to_numpy())\n",
    "    total_array = np.concatenate(arrays)\n",
    "    mean = total_array[:, :8].mean(0)\n",
    "    std = np.nanstd(total_array[:, :8].astype(float), axis=0)\n",
    "    columns = list(df.columns)[:8]\n",
    "    record = {columns[i]: {'mean': mean[i], 'std': std[i]} for i in range(len(columns))}\n",
    "    #print(record)\n",
    "    stats[user_id] = record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_X': {'mean': -0.14136787738247256, 'std': 0.5426113879587267}, 'acc_Y': {'mean': 0.023768131889762057, 'std': 0.4689673054625551}, 'acc_Z': {'mean': -0.012439029132779127, 'std': 0.4406461071283258}, 'gyr_X': {'mean': -0.03419923962560961, 'std': 8.348854037356908}, 'gyr_Y': {'mean': 0.29665035106841764, 'std': 6.749337848374998}, 'gyr_Z': {'mean': 0.04557008292909999, 'std': 6.504126240479325}, 'heartRate': {'mean': 88.81403669694498, 'std': 21.499448769234718}, 'rRInterval': {'mean': 713.7407634794719, 'std': 158.07935502391476}}\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path(\"../data/track2/raw\")\n",
    "\n",
    "#with open(output_dir/\"subject_stats.json\", \"w\") as f:\n",
    "#    json.dump(stats, f)\n",
    "\n",
    "with open(output_dir/\"subject_stats.json\", \"r\") as f:\n",
    "    stats = json.load(f)\n",
    "\n",
    "user = '9'\n",
    "print(stats[user])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPreventionDataset(CacheDataset):\n",
    "    def __init__(self, split_path, split, transforms, max_samples=None, subject=None, cache_num = sys.maxsize, cache_rate=1.0, num_workers=1):    \n",
    "        \n",
    "        self.split = split\n",
    "        self.max_samples = max_samples\n",
    "        self.subject = subject\n",
    "        \n",
    "        data = self._generate_data_list(split_path/f\"{split}_dataset.csv\")\n",
    "\n",
    "        super().__init__(data, transforms, cache_num=cache_num, cache_rate=cache_rate, num_workers=num_workers)\n",
    "        \n",
    "     \n",
    "    #split data in train, val and test sets in a reproducible way\n",
    "    def _generate_data_list(self, split_path):\n",
    "\n",
    "        # open csv with observations\n",
    "        data_list = pd.read_csv(split_path, index_col=0, nrows=self.max_samples)\n",
    "        if self.subject is not None:\n",
    "           # filter subject\n",
    "            data_list = data_list[data_list['user_id']==self.subject]\n",
    "        # filter valid\n",
    "        data_list = data_list[data_list.valid.astype(bool)]\n",
    "        # save ditribution\n",
    "        count_distribution = data_list.label.value_counts().sort_index().to_numpy()\n",
    "        num_samples = len(data_list)\n",
    "        self.distribution = count_distribution / num_samples\n",
    "\n",
    "        return data_list.to_dict('records')  \n",
    "    \n",
    "    def get_label_proportions(self):\n",
    "\n",
    "        return self.distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 40/40 [00:00<00:00, 544714.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data_file': 'training_data/user_00/val/non-relapse/00/data.csv',\n",
       " 'user_id': 0,\n",
       " 'sample_id': 0,\n",
       " 'label': 0,\n",
       " 'valid': 1,\n",
       " 'offsets': '[(0, 2160), (2160, 4320), (4320, 6480), (6480, 8640), (8640, 10800), (10800, 12960)]'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = EPreventionDataset(split_path=Path(\"../data/track2/raw\"), split='val', subject=0, transforms=None, max_samples=100)\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 956/956 [01:03<00:00, 15.04it/s]\n"
     ]
    }
   ],
   "source": [
    "class AppendRootDirD(MapTransform):\n",
    "\n",
    "    def __init__(self, keys: KeysCollection, root_dir):\n",
    "        super().__init__(keys)\n",
    "        self.root_dir = root_dir\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d[k] = os.path.join(self.root_dir,d[k])\n",
    "        return d\n",
    "        \n",
    "class LoadDataD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, split, use_sleeping):\n",
    "        super().__init__(keys)\n",
    "        self.split = split\n",
    "        if use_sleeping:\n",
    "            self.cols = ['acc_X', 'acc_Y', 'acc_Z', 'gyr_X', 'gyr_Y', 'gyr_Z', 'heartRate', 'rRInterval', 'timecol', 'sleeping']\n",
    "        else:\n",
    "            self.cols = ['acc_X', 'acc_Y', 'acc_Z', 'gyr_X', 'gyr_Y', 'gyr_Z', 'heartRate', 'rRInterval', 'timecol']\n",
    "\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            if self.split == 'train':\n",
    "                d['data'] = pd.read_csv(d[k],\n",
    "                    skiprows=lambda x : x in range(1, d['start_data_row']+1),\n",
    "                    nrows=d['end_data_row']-d['start_data_row'],\n",
    "                    usecols=self.cols) \n",
    "            else:\n",
    "                d['data'] = pd.read_csv(d[k], usecols=self.cols)\n",
    "            if self.split == 'test':\n",
    "                d['sample_id'] = d['data_file'].split(\"/\")[-2]\n",
    "            del d[k]\n",
    "        if 'valid' in d.keys(): del d['valid']\n",
    "        if 'start_data_row' in d.keys(): del d['start_data_row']\n",
    "        if 'end_data_row' in d.keys(): del d['end_data_row']\n",
    "        return d\n",
    "\n",
    "class DeleteTimeD(MapTransform):\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            del d[k]\n",
    "        return d\n",
    "\n",
    "class ImputeMedianD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection):\n",
    "        super().__init__(keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            # impute median\n",
    "            d[k] = d[k].replace([np.inf, -np.inf], np.nan)\n",
    "            d[k] = d[k].fillna(d[k].median())\n",
    "            # check whole nan cols\n",
    "            user = str(d['user_id'])\n",
    "            for col in d[k].columns:\n",
    "                if d[k][col].isna().all():\n",
    "                    d[k][col] = stats[user][col]['mean']\n",
    "        return d\n",
    "\n",
    "class ToNumpyD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection):\n",
    "        super().__init__(keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d[k] = d[k].to_numpy()\n",
    "        return d\n",
    "\n",
    "class StandardizeD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection):\n",
    "        super().__init__(keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            user = str(d['user_id'])\n",
    "            means = torch.tensor([stat['mean'] for _, stat in stats[user].items()])\n",
    "            stds = torch.tensor([stat['std'] for _, stat in stats[user].items()])\n",
    "            #means[7:] = 0.\n",
    "            #stds[7:] = 1.\n",
    "            #print(means, stds)\n",
    "            d[k] = (d[k] - means)/stds\n",
    "        return d\n",
    "\n",
    "class TransposeD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection):\n",
    "        super().__init__(keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d[k] = d[k].t()\n",
    "        return d\n",
    "\n",
    "class FlattenD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection):\n",
    "        super().__init__(keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            if len(d[k].shape) == 2:\n",
    "                d[k] = d[k].flatten()\n",
    "            else:\n",
    "                d[k] = d[k].flatten(start_dim=1)\n",
    "        return d\n",
    "\n",
    "class ExtractTimeD(MapTransform):\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        for k in self.keys:\n",
    "            d['time'] = d[k].timecol.astype('datetime64[ns]')\n",
    "            d[k].drop('timecol', inplace=True, axis=1)\n",
    "        return d\n",
    "\n",
    "root_dir = Path(\"../../datasets/SPGC_challenge_track_2_release\")\n",
    "\n",
    "transforms = [\n",
    "        ToTensorD(['label'],dtype=torch.long),\n",
    "        AppendRootDirD(['data_file'], root_dir),\n",
    "        LoadDataD(['data_file'], 'train', use_sleeping=False),\n",
    "        ExtractTimeD(['data']),\n",
    "        DeleteTimeD(['time']),\n",
    "        ImputeMedianD(['data']),\n",
    "        ToNumpyD(['data']),\n",
    "        ToTensorD(['data'], dtype=torch.float),\n",
    "        StandardizeD(['data']),\n",
    "        TransposeD(['data']),\n",
    "]\n",
    "\n",
    "transforms = Compose(transforms)\n",
    "\n",
    "train_data = EPreventionDataset(Path(\"../data/track2/raw\"), 'train', subject=0, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 0,\n",
       " 'sample_id': 37,\n",
       " 'label': tensor(0),\n",
       " 'data': tensor([[ 0.0091,  0.0091,  0.0087,  ...,  0.0066,  0.0064,  0.0064],\n",
       "         [ 0.1761,  0.1379,  0.2219,  ...,  0.0625,  0.0470,  0.0612],\n",
       "         [-0.2325, -0.2580, -0.2016,  ..., -0.2353, -0.2301, -0.2318],\n",
       "         ...,\n",
       "         [-0.0070, -0.0609,  0.0038,  ..., -0.0098, -0.0121, -0.0106],\n",
       "         [-0.4928, -0.4141, -0.3778,  ..., -0.3980, -0.4343, -0.5069],\n",
       "         [ 1.1063,  0.9673,  1.5217,  ...,  1.7636,  1.7084,  1.6666]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Transforms for Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 40/40 [00:10<00:00,  3.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import ConstantPad1d, ReplicationPad1d\n",
    "\n",
    "class PadShortSequenceD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, output_size, padding, mode):\n",
    "        super().__init__(keys)\n",
    "        assert padding in ['replication', 'zero'], \"Select Proper Padding Mode: Allowed same and zero\"\n",
    "        assert mode in ['head', 'center', 'tail'], \"Select Proper Mode: Allowed head, center and tail\"\n",
    "        self.output_size = output_size\n",
    "        self.padding = padding\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        w_in = d['data'].shape[-1]\n",
    "        #print(w_in, self.output_size)\n",
    "        if w_in >= self.output_size:\n",
    "            return d\n",
    "        pad_size = self.output_size - w_in\n",
    "        if self.mode == 'head':\n",
    "            padding = (pad_size, 0)\n",
    "        elif self.mode == 'tail':\n",
    "            padding = (0, pad_size)\n",
    "        elif self.mode == 'center' and pad_size%2==0:\n",
    "            padding = pad_size//2\n",
    "        elif self.mode == 'center' and pad_size%2==1:\n",
    "            padding = (pad_size//2, pad_size//2+1)\n",
    "        pad_fn = self._get_pad_fn(padding)\n",
    "        for k in self.keys:\n",
    "            d[k] = pad_fn(d[k])\n",
    "        return d\n",
    "\n",
    "    def _get_pad_fn(self, padding):\n",
    "        return ConstantPad1d(padding, 0) if self.padding == 'zero' else ReplicationPad1d(padding)\n",
    "\n",
    "class CreateVotingBatchD(MapTransform):\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection):\n",
    "        super().__init__(keys)\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        d = copy.deepcopy(data)\n",
    "        offsets = eval(d['offsets'])\n",
    "        for k in self.keys:\n",
    "            windows = [d[k][:, start:stop].unsqueeze(0) for (start, stop) in offsets]\n",
    "            d[k] = torch.cat(windows, dim=0)\n",
    "        if 'offsets' in d.keys():\n",
    "            del d['offsets']\n",
    "        return d\n",
    "\n",
    "eval_transforms = [\n",
    "        ToTensorD(['label'],dtype=torch.long),\n",
    "        AppendRootDirD(['data_file'], root_dir),\n",
    "        LoadDataD(['data_file'], 'val', use_sleeping=False),\n",
    "        ExtractTimeD(['data']),\n",
    "        DeleteTimeD(['time']),\n",
    "        ImputeMedianD(['data']),\n",
    "        ToNumpyD(['data']),\n",
    "        ToTensorD(['data'], dtype=torch.float),\n",
    "        StandardizeD(['data']),\n",
    "        TransposeD(['data']),\n",
    "        CreateVotingBatchD(['data']),\n",
    "        PadShortSequenceD(['data'], output_size=2160, padding='replication', mode='center'),\n",
    "        #FlattenD(['data'])\n",
    "]\n",
    "\n",
    "eval_transforms = Compose(eval_transforms)\n",
    "\n",
    "val_data = EPreventionDataset(Path(\"../data/track2/raw\"), 'val', subject=0, transforms=eval_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in val_data:\n",
    "    B, F, T = sample['data'].size()\n",
    "    if T<2160:\n",
    "        print(sample['data'].size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "000d80ed2087394a6b578f046794fbe46f974e9842e1e873401be47fee2626f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
